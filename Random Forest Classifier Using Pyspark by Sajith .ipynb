{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:\n",
    "    classification problem, we are trying to predict results in a\n",
    "    discrete output. In other words, we are trying to map input variables into\n",
    "    discrete categories.\n",
    "    \n",
    "    Bank Loan Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Working Directory in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    " \n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Sparksession driver\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Classification of Bank Loan Prediction Dataset \") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+-----+--------+--------+--------+--------+-----------+-----------+---------------+--------------------+------------+--------+------------+\n",
      "| _c0|Age|Experience|Income|CCAvg|Mortgage|Family_2|Family_3|Family_4|Education_2|Education_3|Personal_Loan_1|Securities_Account_1|CD Account_1|Online_1|CreditCard_1|\n",
      "+----+---+----------+------+-----+--------+--------+--------+--------+-----------+-----------+---------------+--------------------+------------+--------+------------+\n",
      "|2764| 31|         5|    84|  2.9|     105|       0|       0|       0|          0|          1|              0|                   0|           0|       0|           1|\n",
      "|4767| 35|         9|    45|  0.9|     101|       0|       1|       0|          0|          0|              0|                   1|           0|       0|           0|\n",
      "|3814| 34|         9|    35|  1.3|       0|       0|       1|       0|          0|          0|              0|                   0|           0|       0|           0|\n",
      "|3499| 49|        23|   114|  0.3|     286|       0|       0|       0|          0|          0|              0|                   0|           0|       1|           0|\n",
      "|2735| 36|        12|    70|  2.6|     165|       0|       1|       0|          1|          0|              0|                   0|           0|       1|           0|\n",
      "|3922| 31|         4|    20|  1.5|       0|       0|       0|       1|          1|          0|              0|                   0|           0|       1|           0|\n",
      "|2701| 50|        26|    55|  1.6|       0|       0|       0|       0|          1|          0|              0|                   0|           0|       1|           0|\n",
      "|1179| 36|        11|    98|  1.2|       0|       0|       1|       0|          0|          1|              0|                   1|           0|       0|           1|\n",
      "| 932| 51|        27|   112|  1.8|       0|       0|       1|       0|          1|          0|              0|                   1|           1|       1|           1|\n",
      "| 792| 41|        16|    98|  4.0|       0|       0|       0|       0|          0|          1|              0|                   0|           0|       0|           1|\n",
      "|1852| 32|         6|    54|  1.8|     167|       0|       0|       1|          0|          1|              0|                   0|           0|       0|           0|\n",
      "|1185| 43|        19|    31|  0.5|       0|       0|       1|       0|          0|          0|              0|                   0|           0|       0|           0|\n",
      "|1724| 46|        19|    24| 0.67|       0|       0|       1|       0|          1|          0|              0|                   0|           0|       1|           0|\n",
      "|4080| 27|         0|    40|  2.0|     110|       0|       0|       0|          1|          0|              0|                   0|           0|       0|           1|\n",
      "|3823| 49|        25|    44|  0.9|     194|       0|       0|       1|          1|          0|              0|                   0|           0|       1|           0|\n",
      "|4054| 59|        34|    64|  1.7|       0|       0|       0|       1|          0|          0|              0|                   0|           0|       0|           0|\n",
      "|2721| 58|        33|   173|  7.2|       0|       1|       0|       0|          0|          1|              1|                   0|           0|       1|           0|\n",
      "|3903| 47|        23|    65|  0.0|       0|       0|       0|       0|          0|          0|              0|                   0|           0|       0|           0|\n",
      "|1865| 36|         6|    90|  1.8|       0|       0|       0|       1|          0|          1|              0|                   1|           0|       0|           0|\n",
      "| 759| 53|        28|    59|  1.9|       0|       1|       0|       0|          1|          0|              0|                   0|           0|       1|           0|\n",
      "+----+---+----------+------+-----+--------+--------+--------+--------+-----------+-----------+---------------+--------------------+------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('logistic/classificationspark.csv',inferSchema=True,header = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of cells in column _c0 with null values: 0\n",
      "no. of cells in column Age with null values: 0\n",
      "no. of cells in column Experience with null values: 0\n",
      "no. of cells in column Income with null values: 0\n",
      "no. of cells in column CCAvg with null values: 0\n",
      "no. of cells in column Mortgage with null values: 0\n",
      "no. of cells in column Family_2 with null values: 0\n",
      "no. of cells in column Family_3 with null values: 0\n",
      "no. of cells in column Family_4 with null values: 0\n",
      "no. of cells in column Education_2 with null values: 0\n",
      "no. of cells in column Education_3 with null values: 0\n",
      "no. of cells in column Personal_Loan_1 with null values: 0\n",
      "no. of cells in column Securities_Account_1 with null values: 0\n",
      "no. of cells in column CD Account_1 with null values: 0\n",
      "no. of cells in column Online_1 with null values: 0\n",
      "no. of cells in column CreditCard_1 with null values: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "for col in df.columns:\n",
    "    print(\"no. of cells in column\", col, \"with null values:\", df.filter(df[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|Personal_Loan_1|\n",
      "+--------------------+---------------+\n",
      "|(14,[0,1,2,3,4,9,...|              0|\n",
      "|(14,[0,1,2,3,4,6,...|              0|\n",
      "|(14,[0,1,2,3,6],[...|              0|\n",
      "|(14,[0,1,2,3,4,12...|              0|\n",
      "|(14,[0,1,2,3,4,6,...|              0|\n",
      "+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all the independent variables need to be packed into one column of vector type\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Age','Experience','Income','CCAvg','Mortgage','Family_2','Family_3','Family_4','Education_2',\n",
    "                                       'Education_3','Securities_Account_1','CD Account_1','Online_1','CreditCard_1'], \n",
    "                            outputCol=\"features\")\n",
    "feature_vec=assembler.transform(df).select('features','Personal_Loan_1')\n",
    "feature_vec.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Personal_Loan_1|count|\n",
      "+---------------+-----+\n",
      "|              1|  480|\n",
      "|              0| 4520|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count of target classes\n",
    "feature_vec.groupBy('Personal_Loan_1').count().show()\n",
    "#there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = feature_vec.randomSplit([.75,.25],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#Grid Search\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "rf = RandomForestClassifier( labelCol='Personal_Loan_1',seed=0)\n",
    "paramGrid = (ParamGridBuilder()\\\n",
    "             .addGrid(rf.maxDepth,[10,11,12])\\\n",
    "             .addGrid(rf.numTrees,[20,30,40])\\\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='Personal_Loan_1', metricName='f1')\n",
    "# Create 4-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9838957430065822,\n",
       " {Param(parent='RandomForestClassifier_4bd98dd525c2f3495467', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 11,\n",
       "  Param(parent='RandomForestClassifier_4bd98dd525c2f3495467', name='numTrees', doc='Number of trees to train (>= 1).'): 30})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Model Params\n",
    "score_params_list = list(zip(cvModel.avgMetrics, cvModel.getEstimatorParamMaps()))\n",
    "max(score_params_list,key=lambda item:item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9806296571649721"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.transform(test_data)\n",
    "evaluator.evaluate(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
